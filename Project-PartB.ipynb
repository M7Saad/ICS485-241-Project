{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part-B, Active Learning\n",
        "\n",
        "- used resources:\n",
        "\n",
        "  1 - https://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_digits_active_learning.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-digits-active-learning-py\n",
        "\n",
        "  2 - https://medium.com/@hardik.dave/active-learning-sampling-strategies-f8d8ac7037c8-\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>x14</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sample</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1353</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.142857</td>\n",
              "      <td>88</td>\n",
              "      <td>104.850</td>\n",
              "      <td>0.00727</td>\n",
              "      <td>0.443</td>\n",
              "      <td>7.997</td>\n",
              "      <td>6.990</td>\n",
              "      <td>8346.00</td>\n",
              "      <td>3.9</td>\n",
              "      <td>0.032695</td>\n",
              "      <td>0.05</td>\n",
              "      <td>C1</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1107</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.857143</td>\n",
              "      <td>306</td>\n",
              "      <td>194.175</td>\n",
              "      <td>0.03778</td>\n",
              "      <td>0.363</td>\n",
              "      <td>34.002</td>\n",
              "      <td>12.945</td>\n",
              "      <td>376.64</td>\n",
              "      <td>11.1</td>\n",
              "      <td>0.210526</td>\n",
              "      <td>3.15</td>\n",
              "      <td>C3</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>984</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.571429</td>\n",
              "      <td>368</td>\n",
              "      <td>208.575</td>\n",
              "      <td>0.05750</td>\n",
              "      <td>0.356</td>\n",
              "      <td>46.000</td>\n",
              "      <td>13.905</td>\n",
              "      <td>451.54</td>\n",
              "      <td>13.1</td>\n",
              "      <td>0.271930</td>\n",
              "      <td>3.10</td>\n",
              "      <td>C1</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1107</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>297</td>\n",
              "      <td>175.725</td>\n",
              "      <td>0.03667</td>\n",
              "      <td>0.354</td>\n",
              "      <td>33.003</td>\n",
              "      <td>11.715</td>\n",
              "      <td>393.76</td>\n",
              "      <td>10.5</td>\n",
              "      <td>0.185008</td>\n",
              "      <td>2.85</td>\n",
              "      <td>C1</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>123</td>\n",
              "      <td>NaN</td>\n",
              "      <td>33.571429</td>\n",
              "      <td>235</td>\n",
              "      <td>225.000</td>\n",
              "      <td>2.35000</td>\n",
              "      <td>0.923</td>\n",
              "      <td>235.000</td>\n",
              "      <td>15.000</td>\n",
              "      <td>5805.82</td>\n",
              "      <td>21.7</td>\n",
              "      <td>0.187400</td>\n",
              "      <td>0.40</td>\n",
              "      <td>C4</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          x1  x2         x3   x4       x5       x6     x7       x8      x9  \\\n",
              "sample                                                                       \n",
              "1       1353 NaN   1.142857   88  104.850  0.00727  0.443    7.997   6.990   \n",
              "2       1107 NaN   4.857143  306  194.175  0.03778  0.363   34.002  12.945   \n",
              "3        984 NaN   6.571429  368  208.575  0.05750  0.356   46.000  13.905   \n",
              "4       1107 NaN   4.714286  297  175.725  0.03667  0.354   33.003  11.715   \n",
              "5        123 NaN  33.571429  235  225.000  2.35000  0.923  235.000  15.000   \n",
              "\n",
              "            x10   x11       x12   x13 x14     y  \n",
              "sample                                           \n",
              "1       8346.00   3.9  0.032695  0.05  C1   Low  \n",
              "2        376.64  11.1  0.210526  3.15  C3   Low  \n",
              "3        451.54  13.1  0.271930  3.10  C1   Low  \n",
              "4        393.76  10.5  0.185008  2.85  C1   Low  \n",
              "5       5805.82  21.7  0.187400  0.40  C4  High  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "from scipy.stats import entropy as entropy_test\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "SEED = 485\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv(\"Dataset-train-vf.csv\", index_col=\"sample\")\n",
        "\n",
        "# Display the first few rows\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocess the data\n",
        "- Remove the columns with more than 50% missing values and impute the rest with mean\n",
        "- convert the categorical columns into numerical columns\n",
        "- scale the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>y</th>\n",
              "      <th>x14_C1</th>\n",
              "      <th>x14_C2</th>\n",
              "      <th>x14_C3</th>\n",
              "      <th>x14_C4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sample</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.103775</td>\n",
              "      <td>-0.646737</td>\n",
              "      <td>-0.174571</td>\n",
              "      <td>-1.558357</td>\n",
              "      <td>-0.429737</td>\n",
              "      <td>-0.051848</td>\n",
              "      <td>-0.512699</td>\n",
              "      <td>-1.558357</td>\n",
              "      <td>0.135166</td>\n",
              "      <td>-0.220630</td>\n",
              "      <td>-0.268636</td>\n",
              "      <td>-0.379805</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.153380</td>\n",
              "      <td>-0.405987</td>\n",
              "      <td>-0.152026</td>\n",
              "      <td>0.318840</td>\n",
              "      <td>-0.370081</td>\n",
              "      <td>-0.372019</td>\n",
              "      <td>-0.329673</td>\n",
              "      <td>0.318840</td>\n",
              "      <td>-0.098963</td>\n",
              "      <td>-0.183989</td>\n",
              "      <td>-0.189301</td>\n",
              "      <td>-0.095592</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.178183</td>\n",
              "      <td>-0.294872</td>\n",
              "      <td>-0.145615</td>\n",
              "      <td>0.621461</td>\n",
              "      <td>-0.331523</td>\n",
              "      <td>-0.400034</td>\n",
              "      <td>-0.245230</td>\n",
              "      <td>0.621461</td>\n",
              "      <td>-0.096762</td>\n",
              "      <td>-0.173811</td>\n",
              "      <td>-0.161907</td>\n",
              "      <td>-0.100176</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.153380</td>\n",
              "      <td>-0.415247</td>\n",
              "      <td>-0.152957</td>\n",
              "      <td>-0.068894</td>\n",
              "      <td>-0.372252</td>\n",
              "      <td>-0.408039</td>\n",
              "      <td>-0.336704</td>\n",
              "      <td>-0.068894</td>\n",
              "      <td>-0.098460</td>\n",
              "      <td>-0.187043</td>\n",
              "      <td>-0.200685</td>\n",
              "      <td>-0.123096</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.351803</td>\n",
              "      <td>1.455192</td>\n",
              "      <td>-0.159369</td>\n",
              "      <td>0.966638</td>\n",
              "      <td>4.150955</td>\n",
              "      <td>1.869182</td>\n",
              "      <td>1.084974</td>\n",
              "      <td>0.966638</td>\n",
              "      <td>0.060539</td>\n",
              "      <td>-0.130046</td>\n",
              "      <td>-0.199618</td>\n",
              "      <td>-0.347717</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              x1        x3        x4        x5        x6        x7        x8  \\\n",
              "sample                                                                         \n",
              "1      -0.103775 -0.646737 -0.174571 -1.558357 -0.429737 -0.051848 -0.512699   \n",
              "2      -0.153380 -0.405987 -0.152026  0.318840 -0.370081 -0.372019 -0.329673   \n",
              "3      -0.178183 -0.294872 -0.145615  0.621461 -0.331523 -0.400034 -0.245230   \n",
              "4      -0.153380 -0.415247 -0.152957 -0.068894 -0.372252 -0.408039 -0.336704   \n",
              "5      -0.351803  1.455192 -0.159369  0.966638  4.150955  1.869182  1.084974   \n",
              "\n",
              "              x9       x10       x11       x12       x13  y  x14_C1  x14_C2  \\\n",
              "sample                                                                        \n",
              "1      -1.558357  0.135166 -0.220630 -0.268636 -0.379805  0    True   False   \n",
              "2       0.318840 -0.098963 -0.183989 -0.189301 -0.095592  0   False   False   \n",
              "3       0.621461 -0.096762 -0.173811 -0.161907 -0.100176  0    True   False   \n",
              "4      -0.068894 -0.098460 -0.187043 -0.200685 -0.123096  0    True   False   \n",
              "5       0.966638  0.060539 -0.130046 -0.199618 -0.347717  1   False   False   \n",
              "\n",
              "        x14_C3  x14_C4  \n",
              "sample                  \n",
              "1        False   False  \n",
              "2         True   False  \n",
              "3        False   False  \n",
              "4        False   False  \n",
              "5        False    True  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "x6_mean = train_data[\"x6\"].mean()\n",
        "# drop 'x2' column because it has a lot of missing values >89%\n",
        "train_data = train_data.drop([\"x2\"], axis=1)\n",
        "\n",
        "# impute missing values with the mean (x6)\n",
        "train_data[\"x6\"] = train_data[\"x6\"].fillna(train_data[\"x6\"].mean())\n",
        "\n",
        "\n",
        "# scale\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# scale all except the target column named \"y\" and the categorical column named \"x14\"\n",
        "train_data[train_data.columns.difference([\"y\", \"x14\"])] = scaler.fit_transform(\n",
        "    train_data[train_data.columns.difference([\"y\", \"x14\"])]\n",
        ")\n",
        "\n",
        "# convert categorical data to numerical data\n",
        "train_data = pd.get_dummies(train_data, columns=[\"x14\"])\n",
        "\n",
        "# map the target column to 0 and 1\n",
        "train_data[\"y\"] = train_data[\"y\"].map({\"Low\": 0, \"High\": 1})\n",
        "\n",
        "display(train_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- choose 50 random samples and keep them as labeled data, and the rest as unlabeled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Active learning using least confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_least_confident_samples(\n",
        "    model: LogisticRegression, X_unlabeled: pd.DataFrame, n_samples: int\n",
        ") -> np.array:\n",
        "    \"\"\"\n",
        "    Query the least confident samples based on predicted probabilities.\n",
        "\n",
        "    Parameters:\n",
        "        model: Trained LogisticRegression model.\n",
        "        X_unlabeled: pd.DataFrame, unlabeled data.\n",
        "        n_samples: int, number of samples to query.\n",
        "\n",
        "    Returns:\n",
        "        least_confident_indices: np.array, indices of the least confident samples (NOTE: not the sample ID but the index in the DataFrame).\n",
        "    \"\"\"\n",
        "    # calculate probablities\n",
        "    probas = model.predict_proba(X_unlabeled)\n",
        "\n",
        "    # Calculate least confidence as 1 - max probability for each sample\n",
        "    least_confidences = 1 - np.max(probas, axis=1)\n",
        "\n",
        "    # get the least confident samples\n",
        "    least_confident_indices = np.argsort(least_confidences)[-n_samples:]\n",
        "\n",
        "    return least_confident_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>x14_C1</th>\n",
              "      <th>x14_C2</th>\n",
              "      <th>x14_C3</th>\n",
              "      <th>x14_C4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sample</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.103775</td>\n",
              "      <td>-0.646737</td>\n",
              "      <td>-0.174571</td>\n",
              "      <td>-1.558357</td>\n",
              "      <td>-0.429737</td>\n",
              "      <td>-0.051848</td>\n",
              "      <td>-0.512699</td>\n",
              "      <td>-1.558357</td>\n",
              "      <td>0.135166</td>\n",
              "      <td>-0.220630</td>\n",
              "      <td>-0.268636</td>\n",
              "      <td>-0.379805</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.153380</td>\n",
              "      <td>-0.405987</td>\n",
              "      <td>-0.152026</td>\n",
              "      <td>0.318840</td>\n",
              "      <td>-0.370081</td>\n",
              "      <td>-0.372019</td>\n",
              "      <td>-0.329673</td>\n",
              "      <td>0.318840</td>\n",
              "      <td>-0.098963</td>\n",
              "      <td>-0.183989</td>\n",
              "      <td>-0.189301</td>\n",
              "      <td>-0.095592</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.178183</td>\n",
              "      <td>-0.294872</td>\n",
              "      <td>-0.145615</td>\n",
              "      <td>0.621461</td>\n",
              "      <td>-0.331523</td>\n",
              "      <td>-0.400034</td>\n",
              "      <td>-0.245230</td>\n",
              "      <td>0.621461</td>\n",
              "      <td>-0.096762</td>\n",
              "      <td>-0.173811</td>\n",
              "      <td>-0.161907</td>\n",
              "      <td>-0.100176</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.153380</td>\n",
              "      <td>-0.415247</td>\n",
              "      <td>-0.152957</td>\n",
              "      <td>-0.068894</td>\n",
              "      <td>-0.372252</td>\n",
              "      <td>-0.408039</td>\n",
              "      <td>-0.336704</td>\n",
              "      <td>-0.068894</td>\n",
              "      <td>-0.098460</td>\n",
              "      <td>-0.187043</td>\n",
              "      <td>-0.200685</td>\n",
              "      <td>-0.123096</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.351803</td>\n",
              "      <td>1.455192</td>\n",
              "      <td>-0.159369</td>\n",
              "      <td>0.966638</td>\n",
              "      <td>4.150955</td>\n",
              "      <td>1.869182</td>\n",
              "      <td>1.084974</td>\n",
              "      <td>0.966638</td>\n",
              "      <td>0.060539</td>\n",
              "      <td>-0.130046</td>\n",
              "      <td>-0.199618</td>\n",
              "      <td>-0.347717</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              x1        x3        x4        x5        x6        x7        x8  \\\n",
              "sample                                                                         \n",
              "1      -0.103775 -0.646737 -0.174571 -1.558357 -0.429737 -0.051848 -0.512699   \n",
              "2      -0.153380 -0.405987 -0.152026  0.318840 -0.370081 -0.372019 -0.329673   \n",
              "3      -0.178183 -0.294872 -0.145615  0.621461 -0.331523 -0.400034 -0.245230   \n",
              "4      -0.153380 -0.415247 -0.152957 -0.068894 -0.372252 -0.408039 -0.336704   \n",
              "5      -0.351803  1.455192 -0.159369  0.966638  4.150955  1.869182  1.084974   \n",
              "\n",
              "              x9       x10       x11       x12       x13  x14_C1  x14_C2  \\\n",
              "sample                                                                     \n",
              "1      -1.558357  0.135166 -0.220630 -0.268636 -0.379805    True   False   \n",
              "2       0.318840 -0.098963 -0.183989 -0.189301 -0.095592   False   False   \n",
              "3       0.621461 -0.096762 -0.173811 -0.161907 -0.100176    True   False   \n",
              "4      -0.068894 -0.098460 -0.187043 -0.200685 -0.123096    True   False   \n",
              "5       0.966638  0.060539 -0.130046 -0.199618 -0.347717   False   False   \n",
              "\n",
              "        x14_C3  x14_C4  \n",
              "sample                  \n",
              "1        False   False  \n",
              "2         True   False  \n",
              "3        False   False  \n",
              "4        False   False  \n",
              "5        False    True  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# choose 50 random samples from the dataset, 40 for training and 10 for validation\n",
        "# get index of the 50 random samples using pandas sample method\n",
        "\n",
        "labeled_data_train = train_data.sample(n=40, random_state=SEED)\n",
        "\n",
        "# 10 random samples for validation (not used in training) to evaluate the model\n",
        "labeled_data_validation = train_data.drop(labeled_data_train.index).sample(\n",
        "    n=10, random_state=SEED\n",
        ")\n",
        "\n",
        "\n",
        "# unlabeled data is the rest of the data\n",
        "unlabeled_data = (\n",
        "    train_data.drop(labeled_data_train.index)\n",
        "    .drop(labeled_data_validation.index)\n",
        "    .drop(\"y\", axis=1)\n",
        ")\n",
        "\n",
        "display(unlabeled_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8\n",
            "Accuracy: 0.8, Number of training samples: 41\n",
            "Accuracy: 0.8, Number of training samples: 42\n",
            "Accuracy: 0.7, Number of training samples: 43\n",
            "Accuracy: 0.7, Number of training samples: 44\n",
            "Accuracy: 0.6, Number of training samples: 45\n",
            "Accuracy: 0.5, Number of training samples: 46\n",
            "Accuracy: 0.6, Number of training samples: 47\n",
            "Accuracy: 0.6, Number of training samples: 48\n",
            "Accuracy: 0.6, Number of training samples: 49\n",
            "Accuracy: 0.6, Number of training samples: 50\n",
            "Accuracy: 0.7, Number of training samples: 51\n",
            "Accuracy: 0.7, Number of training samples: 52\n",
            "Accuracy: 0.7, Number of training samples: 53\n",
            "Accuracy: 0.8, Number of training samples: 54\n",
            "Accuracy: 0.8, Number of training samples: 55\n",
            "Accuracy: 0.8, Number of training samples: 56\n",
            "Accuracy: 0.8, Number of training samples: 57\n",
            "Accuracy: 0.8, Number of training samples: 58\n",
            "Accuracy: 0.8, Number of training samples: 59\n",
            "Accuracy: 0.7, Number of training samples: 60\n",
            "Accuracy: 0.8, Number of training samples: 61\n",
            "Accuracy: 0.8, Number of training samples: 62\n",
            "Accuracy: 0.8, Number of training samples: 63\n",
            "Accuracy: 0.8, Number of training samples: 64\n",
            "Accuracy: 0.8, Number of training samples: 65\n",
            "Accuracy: 0.8, Number of training samples: 66\n",
            "Accuracy: 0.8, Number of training samples: 67\n",
            "Accuracy: 0.8, Number of training samples: 68\n",
            "Accuracy: 0.8, Number of training samples: 69\n",
            "Accuracy: 0.8, Number of training samples: 70\n",
            "Accuracy: 0.8, Number of training samples: 71\n",
            "Accuracy: 0.8, Number of training samples: 72\n",
            "Accuracy: 0.8, Number of training samples: 73\n",
            "Accuracy: 0.8, Number of training samples: 74\n",
            "Accuracy: 0.8, Number of training samples: 75\n",
            "Accuracy: 0.8, Number of training samples: 76\n",
            "Accuracy: 0.8, Number of training samples: 77\n",
            "Accuracy: 0.8, Number of training samples: 78\n",
            "Accuracy: 0.8, Number of training samples: 79\n",
            "Accuracy: 0.8, Number of training samples: 80\n",
            "Accuracy: 0.8, Number of training samples: 81\n",
            "Accuracy: 0.8, Number of training samples: 82\n",
            "Accuracy: 0.8, Number of training samples: 83\n",
            "Accuracy: 0.8, Number of training samples: 84\n",
            "Accuracy: 0.8, Number of training samples: 85\n",
            "Accuracy: 0.8, Number of training samples: 86\n",
            "Accuracy: 0.8, Number of training samples: 87\n",
            "Accuracy: 0.8, Number of training samples: 88\n",
            "Accuracy: 0.8, Number of training samples: 89\n",
            "Accuracy: 0.8, Number of training samples: 90\n",
            "Accuracy: 0.8, Number of training samples: 91\n",
            "Accuracy: 0.8, Number of training samples: 92\n",
            "Accuracy: 0.8, Number of training samples: 93\n",
            "Accuracy: 0.8, Number of training samples: 94\n",
            "Accuracy: 0.8, Number of training samples: 95\n",
            "Accuracy: 0.8, Number of training samples: 96\n",
            "Accuracy: 0.8, Number of training samples: 97\n",
            "Accuracy: 0.8, Number of training samples: 98\n",
            "Accuracy: 0.8, Number of training samples: 99\n",
            "Accuracy: 0.8, Number of training samples: 100\n",
            "Accuracy: 0.8, Number of training samples: 101\n",
            "Accuracy: 0.8, Number of training samples: 102\n",
            "Accuracy: 0.8, Number of training samples: 103\n",
            "Accuracy: 0.8, Number of training samples: 104\n",
            "Accuracy: 0.8, Number of training samples: 105\n",
            "Accuracy: 0.8, Number of training samples: 106\n",
            "Accuracy: 0.8, Number of training samples: 107\n",
            "Accuracy: 0.8, Number of training samples: 108\n",
            "Accuracy: 0.8, Number of training samples: 109\n",
            "Accuracy: 0.8, Number of training samples: 110\n",
            "Accuracy: 0.8, Number of training samples: 111\n",
            "Accuracy: 0.7, Number of training samples: 112\n",
            "Accuracy: 0.7, Number of training samples: 113\n",
            "Accuracy: 0.7, Number of training samples: 114\n",
            "Accuracy: 0.7, Number of training samples: 115\n",
            "Accuracy: 0.7, Number of training samples: 116\n",
            "Accuracy: 0.7, Number of training samples: 117\n",
            "Accuracy: 0.7, Number of training samples: 118\n",
            "Accuracy: 0.7, Number of training samples: 119\n",
            "Accuracy: 0.7, Number of training samples: 120\n",
            "Accuracy: 0.7, Number of training samples: 121\n",
            "Accuracy: 0.7, Number of training samples: 122\n",
            "Accuracy: 0.7, Number of training samples: 123\n",
            "Accuracy: 0.7, Number of training samples: 124\n",
            "Accuracy: 0.7, Number of training samples: 125\n",
            "Accuracy: 0.7, Number of training samples: 126\n",
            "Accuracy: 0.7, Number of training samples: 127\n",
            "Accuracy: 0.7, Number of training samples: 128\n",
            "Accuracy: 0.7, Number of training samples: 129\n",
            "Accuracy: 0.7, Number of training samples: 130\n",
            "Accuracy: 0.7, Number of training samples: 131\n",
            "Accuracy: 0.7, Number of training samples: 132\n",
            "Accuracy: 0.7, Number of training samples: 133\n",
            "Accuracy: 0.7, Number of training samples: 134\n",
            "Accuracy: 0.7, Number of training samples: 135\n",
            "Accuracy: 0.7, Number of training samples: 136\n",
            "Accuracy: 0.7, Number of training samples: 137\n",
            "Accuracy: 0.7, Number of training samples: 138\n",
            "Accuracy: 0.7, Number of training samples: 139\n",
            "Accuracy: 0.7, Number of training samples: 140\n",
            "Accuracy: 0.7, Number of training samples: 141\n",
            "Accuracy: 0.7, Number of training samples: 142\n",
            "Accuracy: 0.7, Number of training samples: 143\n",
            "Accuracy: 0.7, Number of training samples: 144\n",
            "Accuracy: 0.7, Number of training samples: 145\n",
            "Accuracy: 0.7, Number of training samples: 146\n",
            "Accuracy: 0.7, Number of training samples: 147\n",
            "Accuracy: 0.7, Number of training samples: 148\n",
            "Accuracy: 0.7, Number of training samples: 149\n",
            "Accuracy: 0.7, Number of training samples: 150\n",
            "Accuracy: 0.7, Number of training samples: 151\n",
            "Accuracy: 0.7, Number of training samples: 152\n",
            "Accuracy: 0.7, Number of training samples: 153\n",
            "Accuracy: 0.7, Number of training samples: 154\n",
            "Accuracy: 0.7, Number of training samples: 155\n",
            "Accuracy: 0.7, Number of training samples: 156\n",
            "Accuracy: 0.7, Number of training samples: 157\n",
            "Accuracy: 0.7, Number of training samples: 158\n",
            "Accuracy: 0.7, Number of training samples: 159\n",
            "Accuracy: 0.7, Number of training samples: 160\n",
            "Accuracy: 0.7, Number of training samples: 161\n",
            "Accuracy: 0.7, Number of training samples: 162\n",
            "Accuracy: 0.7, Number of training samples: 163\n",
            "Accuracy: 0.7, Number of training samples: 164\n",
            "Accuracy: 0.7, Number of training samples: 165\n",
            "Accuracy: 0.7, Number of training samples: 166\n",
            "Accuracy: 0.7, Number of training samples: 167\n",
            "Accuracy: 0.7, Number of training samples: 168\n",
            "Accuracy: 0.7, Number of training samples: 169\n",
            "Accuracy: 0.7, Number of training samples: 170\n",
            "Accuracy: 0.7, Number of training samples: 171\n",
            "Accuracy: 0.7, Number of training samples: 172\n",
            "Accuracy: 0.7, Number of training samples: 173\n",
            "Accuracy: 0.7, Number of training samples: 174\n",
            "Accuracy: 0.7, Number of training samples: 175\n",
            "Accuracy: 0.7, Number of training samples: 176\n",
            "Accuracy: 0.7, Number of training samples: 177\n",
            "Accuracy: 0.7, Number of training samples: 178\n",
            "Accuracy: 0.7, Number of training samples: 179\n",
            "Accuracy: 0.7, Number of training samples: 180\n",
            "Accuracy: 0.7, Number of training samples: 181\n",
            "Accuracy: 0.7, Number of training samples: 182\n",
            "Accuracy: 0.7, Number of training samples: 183\n",
            "Accuracy: 0.7, Number of training samples: 184\n",
            "Accuracy: 0.7, Number of training samples: 185\n",
            "Accuracy: 0.7, Number of training samples: 186\n",
            "Accuracy: 0.7, Number of training samples: 187\n",
            "Accuracy: 0.7, Number of training samples: 188\n",
            "Accuracy: 0.7, Number of training samples: 189\n",
            "Accuracy: 0.7, Number of training samples: 190\n",
            "Accuracy: 0.7, Number of training samples: 191\n",
            "Accuracy: 0.7, Number of training samples: 192\n",
            "Accuracy: 0.7, Number of training samples: 193\n",
            "Accuracy: 0.7, Number of training samples: 194\n",
            "Accuracy: 0.7, Number of training samples: 195\n",
            "Accuracy: 0.7, Number of training samples: 196\n",
            "Accuracy: 0.7, Number of training samples: 197\n",
            "Accuracy: 0.7, Number of training samples: 198\n",
            "Accuracy: 0.7, Number of training samples: 199\n",
            "Accuracy: 0.7, Number of training samples: 200\n",
            "Accuracy: 0.7, Number of training samples: 201\n",
            "Accuracy: 0.7, Number of training samples: 202\n",
            "Accuracy: 0.7, Number of training samples: 203\n",
            "Accuracy: 0.7, Number of training samples: 204\n",
            "Accuracy: 0.7, Number of training samples: 205\n",
            "Accuracy: 0.7, Number of training samples: 206\n",
            "Accuracy: 0.7, Number of training samples: 207\n",
            "Accuracy: 0.7, Number of training samples: 208\n",
            "Accuracy: 0.7, Number of training samples: 209\n",
            "Accuracy: 0.7, Number of training samples: 210\n",
            "Accuracy: 0.7, Number of training samples: 211\n",
            "Accuracy: 0.7, Number of training samples: 212\n",
            "Accuracy: 0.8, Number of training samples: 213\n",
            "Accuracy: 0.8, Number of training samples: 214\n",
            "Accuracy: 0.8, Number of training samples: 215\n",
            "Accuracy: 0.8, Number of training samples: 216\n",
            "Accuracy: 0.8, Number of training samples: 217\n",
            "Accuracy: 0.7, Number of training samples: 218\n",
            "Accuracy: 0.7, Number of training samples: 219\n",
            "Accuracy: 0.7, Number of training samples: 220\n",
            "Accuracy: 0.7, Number of training samples: 221\n",
            "Accuracy: 0.7, Number of training samples: 222\n",
            "Accuracy: 0.7, Number of training samples: 223\n",
            "Accuracy: 0.7, Number of training samples: 224\n",
            "Accuracy: 0.6, Number of training samples: 225\n",
            "Accuracy: 0.6, Number of training samples: 226\n",
            "Accuracy: 0.7, Number of training samples: 227\n",
            "Accuracy: 0.7, Number of training samples: 228\n",
            "Accuracy: 0.7, Number of training samples: 229\n",
            "Accuracy: 0.7, Number of training samples: 230\n",
            "Accuracy: 0.7, Number of training samples: 231\n",
            "Accuracy: 0.7, Number of training samples: 232\n",
            "Accuracy: 0.7, Number of training samples: 233\n",
            "Accuracy: 0.7, Number of training samples: 234\n",
            "Accuracy: 0.7, Number of training samples: 235\n",
            "Accuracy: 0.8, Number of training samples: 236\n",
            "Accuracy: 0.7, Number of training samples: 237\n",
            "Accuracy: 0.7, Number of training samples: 238\n",
            "Accuracy: 0.7, Number of training samples: 239\n",
            "Accuracy: 0.7, Number of training samples: 240\n",
            "Accuracy: 0.7, Number of training samples: 241\n",
            "Accuracy: 0.7, Number of training samples: 242\n",
            "Accuracy: 0.8, Number of training samples: 243\n",
            "Accuracy: 0.7, Number of training samples: 244\n",
            "Accuracy: 0.7, Number of training samples: 245\n",
            "Accuracy: 0.7, Number of training samples: 246\n",
            "Accuracy: 0.7, Number of training samples: 247\n",
            "Accuracy: 0.7, Number of training samples: 248\n",
            "Accuracy: 0.7, Number of training samples: 249\n",
            "Accuracy: 0.7, Number of training samples: 250\n",
            "Accuracy: 0.7, Number of training samples: 251\n",
            "Accuracy: 0.7, Number of training samples: 252\n",
            "Accuracy: 0.7, Number of training samples: 253\n",
            "Accuracy: 0.7, Number of training samples: 254\n",
            "Accuracy: 0.8, Number of training samples: 255\n",
            "Accuracy: 0.8, Number of training samples: 256\n",
            "Accuracy: 0.8, Number of training samples: 257\n",
            "Accuracy: 0.8, Number of training samples: 258\n",
            "Accuracy: 0.8, Number of training samples: 259\n",
            "Accuracy: 0.9, Number of training samples: 260\n"
          ]
        }
      ],
      "source": [
        "# logistic regression\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(labeled_data_train.drop(\"y\", axis=1), labeled_data_train[\"y\"])\n",
        "\n",
        "# evaluate the model\n",
        "y_pred = log_reg.predict(labeled_data_validation.drop(\"y\", axis=1))\n",
        "accuracy = accuracy_score(labeled_data_validation[\"y\"], y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "# query the least confident samples while the accuracy on validation data is less than 0.9\n",
        "while accuracy < 0.9 and len(unlabeled_data) > 0:\n",
        "    n_samples = 1\n",
        "    least_confident_indices = query_least_confident_samples(\n",
        "        log_reg, unlabeled_data, n_samples\n",
        "    )\n",
        "\n",
        "    # get the sample IDs of the least confident samples\n",
        "    least_confident_samples = unlabeled_data.iloc[least_confident_indices].index\n",
        "\n",
        "    # get the labels of the least confident samples\n",
        "    # least_confident_labels = train_data.loc[least_confident_samples][\"y\"]\n",
        "\n",
        "    # print(\n",
        "    #    least_confident_labels\n",
        "    # )  # Observation: All the least confident samples are of class 1 (High) for now\n",
        "\n",
        "    # add the least confident samples to the labeled data\n",
        "    labeled_data_train = pd.concat(\n",
        "        [labeled_data_train, train_data.loc[least_confident_samples]]\n",
        "    )\n",
        "\n",
        "    # remove the least confident samples from the unlabeled data\n",
        "    unlabeled_data = unlabeled_data.drop(least_confident_samples)\n",
        "\n",
        "    # retrain the model\n",
        "    log_reg = LogisticRegression()\n",
        "    log_reg.fit(labeled_data_train.drop(\"y\", axis=1), labeled_data_train[\"y\"])\n",
        "\n",
        "    # evaluate the model\n",
        "    y_pred = log_reg.predict(labeled_data_validation.drop(\"y\", axis=1))\n",
        "    accuracy = accuracy_score(labeled_data_validation[\"y\"], y_pred)\n",
        "    print(\n",
        "        f\"Accuracy: {accuracy}, Number of training samples: {len(labeled_data_train)}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "260 samples were used for training.\n",
            "Accuracy on the test set: 0.9037037037037037\n",
            "Confusion matrix:\n",
            "[[244   8]\n",
            " [ 31 122]]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.97      0.93       252\n",
            "           1       0.94      0.80      0.86       153\n",
            "\n",
            "    accuracy                           0.90       405\n",
            "   macro avg       0.91      0.88      0.89       405\n",
            "weighted avg       0.91      0.90      0.90       405\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on the test set\n",
        "test_data = pd.read_csv(\"Dataset-test-vf.csv\", index_col=\"sample\")\n",
        "\n",
        "# drop 'x2' column because it has a lot of missing values >89%\n",
        "test_data = test_data.drop([\"x2\"], axis=1)\n",
        "\n",
        "# impute missing values with the mean (x6) from the training data\n",
        "test_data[\"x6\"] = test_data[\"x6\"].fillna(x6_mean)\n",
        "\n",
        "# scale\n",
        "test_data[test_data.columns.difference([\"y\", \"x14\"])] = scaler.transform(\n",
        "    test_data[test_data.columns.difference([\"y\", \"x14\"])]\n",
        ")\n",
        "\n",
        "# convert categorical data to numerical data\n",
        "test_data = pd.get_dummies(test_data, columns=[\"x14\"])\n",
        "\n",
        "# map the target column to 0 and 1\n",
        "test_data[\"y\"] = test_data[\"y\"].map({\"Low\": 0, \"High\": 1})\n",
        "\n",
        "# evaluate the model on the test set\n",
        "y_pred = log_reg.predict(test_data.drop(\"y\", axis=1))\n",
        "accuracy = accuracy_score(test_data[\"y\"], y_pred)\n",
        "print(f\"{len(labeled_data_train)} samples were used for training.\")\n",
        "print(f\"Accuracy on the test set: {accuracy}\")\n",
        "print(f\"Confusion matrix:\\n{confusion_matrix(test_data['y'], y_pred)}\")\n",
        "print(f\"Classification report:\\n{classification_report(test_data['y'], y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Active learning using entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_entropy_samples(\n",
        "    model: LogisticRegression, X_unlabeled: pd.DataFrame, n_samples: int\n",
        ") -> np.array:\n",
        "    \"\"\"\n",
        "    Query the samples with the highest entropy in predictions.\n",
        "\n",
        "    Parameters:\n",
        "        model: Trained LogisticRegression model.\n",
        "        X_unlabeled: pd.dataframe, feature matrix of unlabeled samples.\n",
        "        n_samples: int, number of samples to query.\n",
        "\n",
        "    Returns:\n",
        "        np.array, indices of the samples with the highest entropy. (NOTE: not the sample ID but the index in the DataFrame).\n",
        "    \"\"\"\n",
        "\n",
        "    # Get predicted probabilities for unlabeled data\n",
        "    probs = model.predict_proba(X_unlabeled)\n",
        "\n",
        "    # Calculate entropy for each sample\n",
        "    entropy = -np.sum(\n",
        "        probs * np.log(probs + 1e-10), axis=1\n",
        "    )  # Add 1e-10 to avoid log(0)\n",
        "\n",
        "    # Get indices of the n_samples with highest entropy\n",
        "    high_entropy_indices = np.argsort(entropy)[-n_samples:]\n",
        "\n",
        "    ############################################################\n",
        "    # Test the implementation\n",
        "    # use entropy api from scipy.stats.entropy to test the implementation\n",
        "    entropy = entropy_test(probs.T)\n",
        "    high_entropy_indices_real = np.argsort(entropy)[-n_samples:]\n",
        "    assert np.array_equal(high_entropy_indices, high_entropy_indices_real)\n",
        "    ############################################################\n",
        "\n",
        "    return high_entropy_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>x14_C1</th>\n",
              "      <th>x14_C2</th>\n",
              "      <th>x14_C3</th>\n",
              "      <th>x14_C4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sample</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.103775</td>\n",
              "      <td>-0.646737</td>\n",
              "      <td>-0.174571</td>\n",
              "      <td>-1.558357</td>\n",
              "      <td>-0.429737</td>\n",
              "      <td>-0.051848</td>\n",
              "      <td>-0.512699</td>\n",
              "      <td>-1.558357</td>\n",
              "      <td>0.135166</td>\n",
              "      <td>-0.220630</td>\n",
              "      <td>-0.268636</td>\n",
              "      <td>-0.379805</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.153380</td>\n",
              "      <td>-0.405987</td>\n",
              "      <td>-0.152026</td>\n",
              "      <td>0.318840</td>\n",
              "      <td>-0.370081</td>\n",
              "      <td>-0.372019</td>\n",
              "      <td>-0.329673</td>\n",
              "      <td>0.318840</td>\n",
              "      <td>-0.098963</td>\n",
              "      <td>-0.183989</td>\n",
              "      <td>-0.189301</td>\n",
              "      <td>-0.095592</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.178183</td>\n",
              "      <td>-0.294872</td>\n",
              "      <td>-0.145615</td>\n",
              "      <td>0.621461</td>\n",
              "      <td>-0.331523</td>\n",
              "      <td>-0.400034</td>\n",
              "      <td>-0.245230</td>\n",
              "      <td>0.621461</td>\n",
              "      <td>-0.096762</td>\n",
              "      <td>-0.173811</td>\n",
              "      <td>-0.161907</td>\n",
              "      <td>-0.100176</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.153380</td>\n",
              "      <td>-0.415247</td>\n",
              "      <td>-0.152957</td>\n",
              "      <td>-0.068894</td>\n",
              "      <td>-0.372252</td>\n",
              "      <td>-0.408039</td>\n",
              "      <td>-0.336704</td>\n",
              "      <td>-0.068894</td>\n",
              "      <td>-0.098460</td>\n",
              "      <td>-0.187043</td>\n",
              "      <td>-0.200685</td>\n",
              "      <td>-0.123096</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.351803</td>\n",
              "      <td>1.455192</td>\n",
              "      <td>-0.159369</td>\n",
              "      <td>0.966638</td>\n",
              "      <td>4.150955</td>\n",
              "      <td>1.869182</td>\n",
              "      <td>1.084974</td>\n",
              "      <td>0.966638</td>\n",
              "      <td>0.060539</td>\n",
              "      <td>-0.130046</td>\n",
              "      <td>-0.199618</td>\n",
              "      <td>-0.347717</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              x1        x3        x4        x5        x6        x7        x8  \\\n",
              "sample                                                                         \n",
              "1      -0.103775 -0.646737 -0.174571 -1.558357 -0.429737 -0.051848 -0.512699   \n",
              "2      -0.153380 -0.405987 -0.152026  0.318840 -0.370081 -0.372019 -0.329673   \n",
              "3      -0.178183 -0.294872 -0.145615  0.621461 -0.331523 -0.400034 -0.245230   \n",
              "4      -0.153380 -0.415247 -0.152957 -0.068894 -0.372252 -0.408039 -0.336704   \n",
              "5      -0.351803  1.455192 -0.159369  0.966638  4.150955  1.869182  1.084974   \n",
              "\n",
              "              x9       x10       x11       x12       x13  x14_C1  x14_C2  \\\n",
              "sample                                                                     \n",
              "1      -1.558357  0.135166 -0.220630 -0.268636 -0.379805    True   False   \n",
              "2       0.318840 -0.098963 -0.183989 -0.189301 -0.095592   False   False   \n",
              "3       0.621461 -0.096762 -0.173811 -0.161907 -0.100176    True   False   \n",
              "4      -0.068894 -0.098460 -0.187043 -0.200685 -0.123096    True   False   \n",
              "5       0.966638  0.060539 -0.130046 -0.199618 -0.347717   False   False   \n",
              "\n",
              "        x14_C3  x14_C4  \n",
              "sample                  \n",
              "1        False   False  \n",
              "2         True   False  \n",
              "3        False   False  \n",
              "4        False   False  \n",
              "5        False    True  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# choose 50 random samples from the dataset, 40 for training and 10 for validation\n",
        "\n",
        "labeled_data_train = train_data.sample(n=40, random_state=SEED)\n",
        "\n",
        "# 10 random samples for validation (not used in training) to evaluate the model\n",
        "labeled_data_validation = train_data.drop(labeled_data_train.index).sample(\n",
        "    n=10, random_state=SEED\n",
        ")\n",
        "\n",
        "# unlabeled data is the rest of the data\n",
        "unlabeled_data = (\n",
        "    train_data.drop(labeled_data_train.index)\n",
        "    .drop(labeled_data_validation.index)\n",
        "    .drop(\"y\", axis=1)\n",
        ")\n",
        "\n",
        "display(unlabeled_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8\n",
            "Accuracy: 0.8, Number of training samples: 41\n",
            "Accuracy: 0.8, Number of training samples: 42\n",
            "Accuracy: 0.7, Number of training samples: 43\n",
            "Accuracy: 0.7, Number of training samples: 44\n",
            "Accuracy: 0.6, Number of training samples: 45\n",
            "Accuracy: 0.5, Number of training samples: 46\n",
            "Accuracy: 0.6, Number of training samples: 47\n",
            "Accuracy: 0.6, Number of training samples: 48\n",
            "Accuracy: 0.6, Number of training samples: 49\n",
            "Accuracy: 0.6, Number of training samples: 50\n",
            "Accuracy: 0.7, Number of training samples: 51\n",
            "Accuracy: 0.7, Number of training samples: 52\n",
            "Accuracy: 0.7, Number of training samples: 53\n",
            "Accuracy: 0.8, Number of training samples: 54\n",
            "Accuracy: 0.8, Number of training samples: 55\n",
            "Accuracy: 0.8, Number of training samples: 56\n",
            "Accuracy: 0.8, Number of training samples: 57\n",
            "Accuracy: 0.8, Number of training samples: 58\n",
            "Accuracy: 0.8, Number of training samples: 59\n",
            "Accuracy: 0.7, Number of training samples: 60\n",
            "Accuracy: 0.8, Number of training samples: 61\n",
            "Accuracy: 0.8, Number of training samples: 62\n",
            "Accuracy: 0.8, Number of training samples: 63\n",
            "Accuracy: 0.8, Number of training samples: 64\n",
            "Accuracy: 0.8, Number of training samples: 65\n",
            "Accuracy: 0.8, Number of training samples: 66\n",
            "Accuracy: 0.8, Number of training samples: 67\n",
            "Accuracy: 0.8, Number of training samples: 68\n",
            "Accuracy: 0.8, Number of training samples: 69\n",
            "Accuracy: 0.8, Number of training samples: 70\n",
            "Accuracy: 0.8, Number of training samples: 71\n",
            "Accuracy: 0.8, Number of training samples: 72\n",
            "Accuracy: 0.8, Number of training samples: 73\n",
            "Accuracy: 0.8, Number of training samples: 74\n",
            "Accuracy: 0.8, Number of training samples: 75\n",
            "Accuracy: 0.8, Number of training samples: 76\n",
            "Accuracy: 0.8, Number of training samples: 77\n",
            "Accuracy: 0.8, Number of training samples: 78\n",
            "Accuracy: 0.8, Number of training samples: 79\n",
            "Accuracy: 0.8, Number of training samples: 80\n",
            "Accuracy: 0.8, Number of training samples: 81\n",
            "Accuracy: 0.8, Number of training samples: 82\n",
            "Accuracy: 0.8, Number of training samples: 83\n",
            "Accuracy: 0.8, Number of training samples: 84\n",
            "Accuracy: 0.8, Number of training samples: 85\n",
            "Accuracy: 0.8, Number of training samples: 86\n",
            "Accuracy: 0.8, Number of training samples: 87\n",
            "Accuracy: 0.8, Number of training samples: 88\n",
            "Accuracy: 0.8, Number of training samples: 89\n",
            "Accuracy: 0.8, Number of training samples: 90\n",
            "Accuracy: 0.8, Number of training samples: 91\n",
            "Accuracy: 0.8, Number of training samples: 92\n",
            "Accuracy: 0.8, Number of training samples: 93\n",
            "Accuracy: 0.8, Number of training samples: 94\n",
            "Accuracy: 0.8, Number of training samples: 95\n",
            "Accuracy: 0.8, Number of training samples: 96\n",
            "Accuracy: 0.8, Number of training samples: 97\n",
            "Accuracy: 0.8, Number of training samples: 98\n",
            "Accuracy: 0.8, Number of training samples: 99\n",
            "Accuracy: 0.8, Number of training samples: 100\n",
            "Accuracy: 0.8, Number of training samples: 101\n",
            "Accuracy: 0.8, Number of training samples: 102\n",
            "Accuracy: 0.8, Number of training samples: 103\n",
            "Accuracy: 0.8, Number of training samples: 104\n",
            "Accuracy: 0.8, Number of training samples: 105\n",
            "Accuracy: 0.8, Number of training samples: 106\n",
            "Accuracy: 0.8, Number of training samples: 107\n",
            "Accuracy: 0.8, Number of training samples: 108\n",
            "Accuracy: 0.8, Number of training samples: 109\n",
            "Accuracy: 0.8, Number of training samples: 110\n",
            "Accuracy: 0.8, Number of training samples: 111\n",
            "Accuracy: 0.7, Number of training samples: 112\n",
            "Accuracy: 0.7, Number of training samples: 113\n",
            "Accuracy: 0.7, Number of training samples: 114\n",
            "Accuracy: 0.7, Number of training samples: 115\n",
            "Accuracy: 0.7, Number of training samples: 116\n",
            "Accuracy: 0.7, Number of training samples: 117\n",
            "Accuracy: 0.7, Number of training samples: 118\n",
            "Accuracy: 0.7, Number of training samples: 119\n",
            "Accuracy: 0.7, Number of training samples: 120\n",
            "Accuracy: 0.7, Number of training samples: 121\n",
            "Accuracy: 0.7, Number of training samples: 122\n",
            "Accuracy: 0.7, Number of training samples: 123\n",
            "Accuracy: 0.7, Number of training samples: 124\n",
            "Accuracy: 0.7, Number of training samples: 125\n",
            "Accuracy: 0.7, Number of training samples: 126\n",
            "Accuracy: 0.7, Number of training samples: 127\n",
            "Accuracy: 0.7, Number of training samples: 128\n",
            "Accuracy: 0.7, Number of training samples: 129\n",
            "Accuracy: 0.7, Number of training samples: 130\n",
            "Accuracy: 0.7, Number of training samples: 131\n",
            "Accuracy: 0.7, Number of training samples: 132\n",
            "Accuracy: 0.7, Number of training samples: 133\n",
            "Accuracy: 0.7, Number of training samples: 134\n",
            "Accuracy: 0.7, Number of training samples: 135\n",
            "Accuracy: 0.7, Number of training samples: 136\n",
            "Accuracy: 0.7, Number of training samples: 137\n",
            "Accuracy: 0.7, Number of training samples: 138\n",
            "Accuracy: 0.7, Number of training samples: 139\n",
            "Accuracy: 0.7, Number of training samples: 140\n",
            "Accuracy: 0.7, Number of training samples: 141\n",
            "Accuracy: 0.7, Number of training samples: 142\n",
            "Accuracy: 0.7, Number of training samples: 143\n",
            "Accuracy: 0.7, Number of training samples: 144\n",
            "Accuracy: 0.7, Number of training samples: 145\n",
            "Accuracy: 0.7, Number of training samples: 146\n",
            "Accuracy: 0.7, Number of training samples: 147\n",
            "Accuracy: 0.7, Number of training samples: 148\n",
            "Accuracy: 0.7, Number of training samples: 149\n",
            "Accuracy: 0.7, Number of training samples: 150\n",
            "Accuracy: 0.7, Number of training samples: 151\n",
            "Accuracy: 0.7, Number of training samples: 152\n",
            "Accuracy: 0.7, Number of training samples: 153\n",
            "Accuracy: 0.7, Number of training samples: 154\n",
            "Accuracy: 0.7, Number of training samples: 155\n",
            "Accuracy: 0.7, Number of training samples: 156\n",
            "Accuracy: 0.7, Number of training samples: 157\n",
            "Accuracy: 0.7, Number of training samples: 158\n",
            "Accuracy: 0.7, Number of training samples: 159\n",
            "Accuracy: 0.7, Number of training samples: 160\n",
            "Accuracy: 0.7, Number of training samples: 161\n",
            "Accuracy: 0.7, Number of training samples: 162\n",
            "Accuracy: 0.7, Number of training samples: 163\n",
            "Accuracy: 0.7, Number of training samples: 164\n",
            "Accuracy: 0.7, Number of training samples: 165\n",
            "Accuracy: 0.7, Number of training samples: 166\n",
            "Accuracy: 0.7, Number of training samples: 167\n",
            "Accuracy: 0.7, Number of training samples: 168\n",
            "Accuracy: 0.7, Number of training samples: 169\n",
            "Accuracy: 0.7, Number of training samples: 170\n",
            "Accuracy: 0.7, Number of training samples: 171\n",
            "Accuracy: 0.7, Number of training samples: 172\n",
            "Accuracy: 0.7, Number of training samples: 173\n",
            "Accuracy: 0.7, Number of training samples: 174\n",
            "Accuracy: 0.7, Number of training samples: 175\n",
            "Accuracy: 0.7, Number of training samples: 176\n",
            "Accuracy: 0.7, Number of training samples: 177\n",
            "Accuracy: 0.7, Number of training samples: 178\n",
            "Accuracy: 0.7, Number of training samples: 179\n",
            "Accuracy: 0.7, Number of training samples: 180\n",
            "Accuracy: 0.7, Number of training samples: 181\n",
            "Accuracy: 0.7, Number of training samples: 182\n",
            "Accuracy: 0.7, Number of training samples: 183\n",
            "Accuracy: 0.7, Number of training samples: 184\n",
            "Accuracy: 0.7, Number of training samples: 185\n",
            "Accuracy: 0.7, Number of training samples: 186\n",
            "Accuracy: 0.7, Number of training samples: 187\n",
            "Accuracy: 0.7, Number of training samples: 188\n",
            "Accuracy: 0.7, Number of training samples: 189\n",
            "Accuracy: 0.7, Number of training samples: 190\n",
            "Accuracy: 0.7, Number of training samples: 191\n",
            "Accuracy: 0.7, Number of training samples: 192\n",
            "Accuracy: 0.7, Number of training samples: 193\n",
            "Accuracy: 0.7, Number of training samples: 194\n",
            "Accuracy: 0.7, Number of training samples: 195\n",
            "Accuracy: 0.7, Number of training samples: 196\n",
            "Accuracy: 0.7, Number of training samples: 197\n",
            "Accuracy: 0.7, Number of training samples: 198\n",
            "Accuracy: 0.7, Number of training samples: 199\n",
            "Accuracy: 0.7, Number of training samples: 200\n",
            "Accuracy: 0.7, Number of training samples: 201\n",
            "Accuracy: 0.7, Number of training samples: 202\n",
            "Accuracy: 0.7, Number of training samples: 203\n",
            "Accuracy: 0.7, Number of training samples: 204\n",
            "Accuracy: 0.7, Number of training samples: 205\n",
            "Accuracy: 0.7, Number of training samples: 206\n",
            "Accuracy: 0.7, Number of training samples: 207\n",
            "Accuracy: 0.7, Number of training samples: 208\n",
            "Accuracy: 0.7, Number of training samples: 209\n",
            "Accuracy: 0.7, Number of training samples: 210\n",
            "Accuracy: 0.7, Number of training samples: 211\n",
            "Accuracy: 0.7, Number of training samples: 212\n",
            "Accuracy: 0.8, Number of training samples: 213\n",
            "Accuracy: 0.8, Number of training samples: 214\n",
            "Accuracy: 0.8, Number of training samples: 215\n",
            "Accuracy: 0.8, Number of training samples: 216\n",
            "Accuracy: 0.8, Number of training samples: 217\n",
            "Accuracy: 0.7, Number of training samples: 218\n",
            "Accuracy: 0.7, Number of training samples: 219\n",
            "Accuracy: 0.7, Number of training samples: 220\n",
            "Accuracy: 0.7, Number of training samples: 221\n",
            "Accuracy: 0.7, Number of training samples: 222\n",
            "Accuracy: 0.7, Number of training samples: 223\n",
            "Accuracy: 0.7, Number of training samples: 224\n",
            "Accuracy: 0.6, Number of training samples: 225\n",
            "Accuracy: 0.6, Number of training samples: 226\n",
            "Accuracy: 0.7, Number of training samples: 227\n",
            "Accuracy: 0.7, Number of training samples: 228\n",
            "Accuracy: 0.7, Number of training samples: 229\n",
            "Accuracy: 0.7, Number of training samples: 230\n",
            "Accuracy: 0.7, Number of training samples: 231\n",
            "Accuracy: 0.7, Number of training samples: 232\n",
            "Accuracy: 0.7, Number of training samples: 233\n",
            "Accuracy: 0.7, Number of training samples: 234\n",
            "Accuracy: 0.7, Number of training samples: 235\n",
            "Accuracy: 0.8, Number of training samples: 236\n",
            "Accuracy: 0.7, Number of training samples: 237\n",
            "Accuracy: 0.7, Number of training samples: 238\n",
            "Accuracy: 0.7, Number of training samples: 239\n",
            "Accuracy: 0.7, Number of training samples: 240\n",
            "Accuracy: 0.7, Number of training samples: 241\n",
            "Accuracy: 0.7, Number of training samples: 242\n",
            "Accuracy: 0.8, Number of training samples: 243\n",
            "Accuracy: 0.7, Number of training samples: 244\n",
            "Accuracy: 0.7, Number of training samples: 245\n",
            "Accuracy: 0.7, Number of training samples: 246\n",
            "Accuracy: 0.7, Number of training samples: 247\n",
            "Accuracy: 0.7, Number of training samples: 248\n",
            "Accuracy: 0.7, Number of training samples: 249\n",
            "Accuracy: 0.7, Number of training samples: 250\n",
            "Accuracy: 0.7, Number of training samples: 251\n",
            "Accuracy: 0.7, Number of training samples: 252\n",
            "Accuracy: 0.7, Number of training samples: 253\n",
            "Accuracy: 0.7, Number of training samples: 254\n",
            "Accuracy: 0.8, Number of training samples: 255\n",
            "Accuracy: 0.8, Number of training samples: 256\n",
            "Accuracy: 0.8, Number of training samples: 257\n",
            "Accuracy: 0.8, Number of training samples: 258\n",
            "Accuracy: 0.8, Number of training samples: 259\n",
            "Accuracy: 0.9, Number of training samples: 260\n"
          ]
        }
      ],
      "source": [
        "# logistic regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(labeled_data_train.drop(\"y\", axis=1), labeled_data_train[\"y\"])\n",
        "\n",
        "# evaluate the model\n",
        "y_pred = log_reg.predict(labeled_data_validation.drop(\"y\", axis=1))\n",
        "accuracy = accuracy_score(labeled_data_validation[\"y\"], y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "# query the most entropic samples while the accuracy on validation data is less than 0.9, and retrain the model\n",
        "while accuracy < 0.9 and len(unlabeled_data) > 0:\n",
        "    n_samples = 1\n",
        "    most_entropic_indices = query_entropy_samples(log_reg, unlabeled_data, n_samples)\n",
        "\n",
        "    # get the sample IDs of the most entropic samples\n",
        "    most_entropic_samples = unlabeled_data.iloc[most_entropic_indices].index\n",
        "\n",
        "    # get the labels of the most entropic samples\n",
        "    # most_entropic_labels = train_data.loc[most_entropic_sample][\"y\"]\n",
        "    # print(\n",
        "    #    most_entropic_labels\n",
        "    # )  # Observation: All the samples are of class 1 (High)\n",
        "\n",
        "    # add the most entropic sample to the labeled data\n",
        "    labeled_data_train = pd.concat(\n",
        "        [labeled_data_train, train_data.loc[most_entropic_samples]]\n",
        "    )\n",
        "\n",
        "    # remove the most entropic samples from the unlabeled data\n",
        "    unlabeled_data = unlabeled_data.drop(most_entropic_samples)\n",
        "\n",
        "    # retrain the model\n",
        "    log_reg = LogisticRegression()\n",
        "    log_reg.fit(labeled_data_train.drop(\"y\", axis=1), labeled_data_train[\"y\"])\n",
        "\n",
        "    # evaluate the model\n",
        "    y_pred = log_reg.predict(labeled_data_validation.drop(\"y\", axis=1))\n",
        "    accuracy = accuracy_score(labeled_data_validation[\"y\"], y_pred)\n",
        "    print(\n",
        "        f\"Accuracy: {accuracy}, Number of training samples: {len(labeled_data_train)}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "260 samples were used for training.\n",
            "Accuracy on the test set: 0.9037037037037037\n",
            "Confusion matrix:\n",
            "[[244   8]\n",
            " [ 31 122]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.97      0.93       252\n",
            "           1       0.94      0.80      0.86       153\n",
            "\n",
            "    accuracy                           0.90       405\n",
            "   macro avg       0.91      0.88      0.89       405\n",
            "weighted avg       0.91      0.90      0.90       405\n",
            "\n",
            "Gmean: 0.878676158592934\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on the test set\n",
        "test_data = pd.read_csv(\"Dataset-test-vf.csv\", index_col=\"sample\")\n",
        "\n",
        "# drop 'x2' column because it has a lot of missing values >89%\n",
        "test_data = test_data.drop([\"x2\"], axis=1)\n",
        "\n",
        "# impute missing values with the mean (x6) from the training data\n",
        "test_data[\"x6\"] = test_data[\"x6\"].fillna(x6_mean)\n",
        "\n",
        "# scale\n",
        "test_data[test_data.columns.difference([\"y\", \"x14\"])] = scaler.transform(\n",
        "    test_data[test_data.columns.difference([\"y\", \"x14\"])]\n",
        ")\n",
        "\n",
        "# convert categorical data to numerical data\n",
        "test_data = pd.get_dummies(test_data, columns=[\"x14\"])\n",
        "\n",
        "# map the target column to 0 and 1\n",
        "test_data[\"y\"] = test_data[\"y\"].map({\"Low\": 0, \"High\": 1})\n",
        "\n",
        "# evaluate the model on the test set\n",
        "y_pred = log_reg.predict(test_data.drop(\"y\", axis=1))\n",
        "accuracy = accuracy_score(test_data[\"y\"], y_pred)\n",
        "print(f\"{len(labeled_data_train)} samples were used for training.\")\n",
        "print(f\"Accuracy on the test set: {accuracy}\")\n",
        "print(f\"Confusion matrix:\\n{confusion_matrix(test_data['y'], y_pred)}\")\n",
        "# recall, precision, f1-score\n",
        "\n",
        "print(classification_report(test_data[\"y\"], y_pred))\n",
        "\n",
        "# Gmean\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "\n",
        "gmean = geometric_mean_score(test_data[\"y\"], y_pred)\n",
        "print(f\"Gmean: {gmean}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using active learning, we trained the model with two strategies: least confidence and entropy sampling. The test accuracy achieved with these methods was  approximately 90%. In both approaches, the model was able to achieve a similar performance level with a significantly reduced amount of labeled data compared to the fully supervised model (only 260 + 10 (validation) samples were labeled instead of 1000). Astonishing! :))\n",
        "\n",
        "\n",
        "# This demonstrates the effectiveness of active learning in significantly reducing the amount of labeled data needed to train a model. Additionally, both the entropy-based/least confidence strategies were effective in selecting the most informative samples for labeling, leading to a more efficient learning process."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
